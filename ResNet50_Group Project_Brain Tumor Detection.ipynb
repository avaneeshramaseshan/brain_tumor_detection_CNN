{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699cc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "from IPython.display import clear_output\n",
    "!pip install kaggle imutils plotly opencv-python matplotlib scikit-learn\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab63f823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaf244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet import preprocess_input\n",
    "\n",
    "def preprocess_imgs(set_name, img_size):\n",
    "    set_new = []\n",
    "    for img in set_name:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(\n",
    "            img,\n",
    "            dsize=img_size,\n",
    "            interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "        img = preprocess_input(img)\n",
    "        set_new.append(img)\n",
    "    return np.array(set_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4fb258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(X, y, labels_dict, n=50):\n",
    "    \"\"\"\n",
    "    Creates a gridplot for desired number of images (n) from the specified set\n",
    "    \"\"\"\n",
    "    for index in range(len(labels_dict)):\n",
    "        imgs = X[np.argwhere(y == index)][:n]\n",
    "        j = 10\n",
    "        i = int(n/j)\n",
    "\n",
    "        plt.figure(figsize=(15,6))\n",
    "        c = 1\n",
    "        for img in imgs:\n",
    "            plt.subplot(i,j,c)\n",
    "            plt.imshow(img[0])\n",
    "\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            c += 1\n",
    "        plt.suptitle('Tumor: {}'.format(labels_dict[index]))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c22c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_imgs(set_name, add_pixels_value=0, final_size=(224,224)):\n",
    "    set_new = []\n",
    "    \n",
    "    for img in tqdm(set_name):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "        thresh = cv2.erode(thresh, None, iterations=2)\n",
    "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        \n",
    "        if len(cnts) == 0:\n",
    "            # if no contours found, skip this image\n",
    "            continue\n",
    "\n",
    "        c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "        extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "        extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "        extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "        extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "        ADD_PIXELS = add_pixels_value\n",
    "        cropped = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "\n",
    "        cropped_resized = cv2.resize(cropped, final_size, interpolation=cv2.INTER_AREA)\n",
    "        set_new.append(cropped_resized)\n",
    "\n",
    "    return np.array(set_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23974f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_path, img_size=(100,100)):\n",
    "    \"\"\"\n",
    "    Load resized images as np.arrays to workspace\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    labels = dict()\n",
    "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
    "        if not path.startswith('.'):\n",
    "            labels[i] = path\n",
    "            for file in os.listdir(os.path.join(dir_path, path)):\n",
    "                if not file.startswith('.'):\n",
    "                    img = cv2.imread(os.path.join(dir_path, path, file))\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        X.append(img)\n",
    "                        y.append(i)\n",
    "            i += 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(f'{len(X)} images loaded from {dir_path} directory.')\n",
    "    return X, y, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    cm = np.round(cm,2)\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae93aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 images loaded from TRAIN/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 36.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 images loaded from TEST/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 40.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 images loaded from VAL/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:00<00:00, 1660.30it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 2043.01it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 2111.11it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'TRAIN/'\n",
    "TEST_DIR = 'TEST/'\n",
    "VAL_DIR = 'VAL/'\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "# use predefined function to load the image data into workspace\n",
    "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
    "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
    "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)\n",
    "X_train_crop = crop_imgs(set_name=X_train)\n",
    "X_val_crop = crop_imgs(set_name=X_val)\n",
    "X_test_crop = crop_imgs(set_name=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0d9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocess_imgs(set_name=X_train_crop, img_size=IMG_SIZE)\n",
    "X_test_prep = preprocess_imgs(set_name=X_test_crop, img_size=IMG_SIZE)\n",
    "X_val_prep = preprocess_imgs(set_name=X_val_crop, img_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c823d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 106 layers, found 448 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load base model\u001b[39;00m\n\u001b[32m      2\u001b[39m resnet50_weight_path = \u001b[33m'\u001b[39m\u001b[33m./keras_pretrained_models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m base_model = \u001b[43mResNet50\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresnet50_weight_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\avane\\Desktop\\brain_tumor_detection_CNN\\myenv-311\\Lib\\site-packages\\keras\\src\\applications\\resnet.py:409\u001b[39m, in \u001b[36mResNet50\u001b[39m\u001b[34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[39m\n\u001b[32m    406\u001b[39m     x = stack_residual_blocks_v1(x, \u001b[32m256\u001b[39m, \u001b[32m6\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33mconv4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stack_residual_blocks_v1(x, \u001b[32m512\u001b[39m, \u001b[32m3\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33mconv5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstack_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreact\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet50\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\avane\\Desktop\\brain_tumor_detection_CNN\\myenv-311\\Lib\\site-packages\\keras\\src\\applications\\resnet.py:214\u001b[39m, in \u001b[36mResNet\u001b[39m\u001b[34m(stack_fn, preact, use_bias, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name, weights_name)\u001b[39m\n\u001b[32m    212\u001b[39m     model.load_weights(weights_path)\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\avane\\Desktop\\brain_tumor_detection_CNN\\myenv-311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\avane\\Desktop\\brain_tumor_detection_CNN\\myenv-311\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:357\u001b[39m, in \u001b[36mload_weights_from_hdf5_group\u001b[39m\u001b[34m(f, model)\u001b[39m\n\u001b[32m    355\u001b[39m layer_names = filtered_layer_names\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) != \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    358\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLayer count mismatch when loading weights from file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    359\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m layers, found \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m saved layers.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(layer_names):\n\u001b[32m    364\u001b[39m     g = f[name]\n",
      "\u001b[31mValueError\u001b[39m: Layer count mismatch when loading weights from file. Model expected 106 layers, found 448 saved layers."
     ]
    }
   ],
   "source": [
    "# load base model\n",
    "resnet50_weight_path = './keras_pretrained_models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "base_model = ResNet50(\n",
    "    weights=resnet50_weight_path,\n",
    "    include_top=False,\n",
    "    input_shape=IMG_SIZE + (3,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1fb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
